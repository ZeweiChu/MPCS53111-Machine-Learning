{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Image Classification\n",
    "\n",
    "Zewei Chu zeweichu@gmail.com\n",
    "\n",
    "Reference\n",
    "- [Stanford CS231n](http://cs231n.github.io/convolutional-networks/)\n",
    "- [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "- [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "- [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- [DenseNet](https://arxiv.org/pdf/1608.06993.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model based on ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1) # 20 * 20\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: 1 * 28 * 28\n",
    "        x = F.relu(self.conv1(x)) # 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # 4 *4 \n",
    "        x = x.view(-1, 4*4*50) # reshape (5 * 2 * 10), view(5, 20) -> (5 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1) # log probability\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./mnist_data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = datasets.MNIST(\"./mnist_data\", train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                           ]))\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [d[0].data.cpu().numpy() for d in mnist_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30810776"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data[223][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLL loss\n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        pred = model(data) # batch_size * 10\n",
    "        loss = F.nll_loss(pred, target) \n",
    "        \n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {}, Loss: {}\".format(\n",
    "                epoch, idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data) # batch_size * 10\n",
    "            total_loss += F.nll_loss(output, target, reduction=\"sum\").item() \n",
    "            pred = output.argmax(dim=1) # batch_size * 1\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    acc = correct/len(test_loader.dataset) * 100.\n",
    "    print(\"Test loss: {}, Accuracy: {}\".format(total_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, Loss: 2.3180577754974365\n",
      "Train Epoch: 0, iteration: 100, Loss: 0.5994296669960022\n",
      "Train Epoch: 0, iteration: 200, Loss: 0.24825681746006012\n",
      "Train Epoch: 0, iteration: 300, Loss: 0.3089035749435425\n",
      "Train Epoch: 0, iteration: 400, Loss: 0.4593719244003296\n",
      "Train Epoch: 0, iteration: 500, Loss: 0.2870183289051056\n",
      "Train Epoch: 0, iteration: 600, Loss: 0.1864209622144699\n",
      "Train Epoch: 0, iteration: 700, Loss: 0.1103847548365593\n",
      "Train Epoch: 0, iteration: 800, Loss: 0.07073946297168732\n",
      "Train Epoch: 0, iteration: 900, Loss: 0.34052252769470215\n",
      "Train Epoch: 0, iteration: 1000, Loss: 0.21388737857341766\n",
      "Train Epoch: 0, iteration: 1100, Loss: 0.10509373992681503\n",
      "Train Epoch: 0, iteration: 1200, Loss: 0.15032045543193817\n",
      "Train Epoch: 0, iteration: 1300, Loss: 0.03676900267601013\n",
      "Train Epoch: 0, iteration: 1400, Loss: 0.14294494688510895\n",
      "Train Epoch: 0, iteration: 1500, Loss: 0.2562383711338043\n",
      "Train Epoch: 0, iteration: 1600, Loss: 0.005333080887794495\n",
      "Train Epoch: 0, iteration: 1700, Loss: 0.03769887983798981\n",
      "Train Epoch: 0, iteration: 1800, Loss: 0.15985722839832306\n",
      "Test loss: 0.08269406108856202, Accuracy: 97.24000000000001\n",
      "Train Epoch: 1, iteration: 0, Loss: 0.06976459920406342\n",
      "Train Epoch: 1, iteration: 100, Loss: 0.1865699291229248\n",
      "Train Epoch: 1, iteration: 200, Loss: 0.09923091530799866\n",
      "Train Epoch: 1, iteration: 300, Loss: 0.31977570056915283\n",
      "Train Epoch: 1, iteration: 400, Loss: 0.1677323877811432\n",
      "Train Epoch: 1, iteration: 500, Loss: 0.05378006398677826\n",
      "Train Epoch: 1, iteration: 600, Loss: 0.1573038250207901\n",
      "Train Epoch: 1, iteration: 700, Loss: 0.005744948983192444\n",
      "Train Epoch: 1, iteration: 800, Loss: 0.004791483283042908\n",
      "Train Epoch: 1, iteration: 900, Loss: 0.1810738444328308\n",
      "Train Epoch: 1, iteration: 1000, Loss: 0.018099471926689148\n",
      "Train Epoch: 1, iteration: 1100, Loss: 0.01936665177345276\n",
      "Train Epoch: 1, iteration: 1200, Loss: 0.19597768783569336\n",
      "Train Epoch: 1, iteration: 1300, Loss: 0.02876429259777069\n",
      "Train Epoch: 1, iteration: 1400, Loss: 0.007609128952026367\n",
      "Train Epoch: 1, iteration: 1500, Loss: 0.020051568746566772\n",
      "Train Epoch: 1, iteration: 1600, Loss: 0.019008994102478027\n",
      "Train Epoch: 1, iteration: 1700, Loss: 0.006903991103172302\n",
      "Train Epoch: 1, iteration: 1800, Loss: 0.005939975380897522\n",
      "Test loss: 0.04865380079746246, Accuracy: 98.34\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", train=True, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ])),\n",
    "    batch_size=batch_size, shuffle=True, \n",
    "    num_workers=1, pin_memory=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", train=False, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))\n",
    "           ])),\n",
    "    batch_size=batch_size, shuffle=True, \n",
    "    num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "lr = 0.01\n",
    "momentum  = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "    \n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Torch Vision Models](https://github.com/pytorch/vision/tree/master/torchvision/models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
